{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from joblib import Parallel, delayed, parallel_config, parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from stream_cat_config import (\n",
    "    LYR_DIR,\n",
    "    MASK_DIR_RP100,\n",
    "    MASK_DIR_SLP10,\n",
    "    MASK_DIR_SLP20,\n",
    "    ACCUM_DIR,\n",
    "    NHD_DIR,\n",
    "    OUT_DIR,\n",
    "    PCT_FULL_FILE,\n",
    "    PCT_FULL_FILE_RP100\n",
    ")\n",
    "\n",
    "from StreamCat_functions import (\n",
    "    PointInPoly,\n",
    "    createCatStats,\n",
    "    mask_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=8, threads_per_worker=2)\n",
    "client = Client(cluster)\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl = pd.read_csv('../ControlTable_StreamCat.csv')\n",
    "inter_vpu = pd.read_csv(\"../InterVPU.csv\")\n",
    "INPUTS = np.load(ACCUM_DIR +\"/vpu_inputs.npy\", allow_pickle=True).item()\n",
    "\n",
    "already_processed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INPUTS)\n",
    "print(len(INPUTS))\n",
    "print(len(ctl.loc[ctl['run'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with parallel_config(backend=\"dask\"):\n",
    "#     Parallel(verbose=100)(\n",
    "#         delayed(long_running_function)(i) for i in range(10)\n",
    "#     )\n",
    "# with parallel_config(backend=\"dask\"):\n",
    "    #     zone_results = Parallel(verbose=100)(\n",
    "    #         delayed(process_zone) (zone, hydroregion) for zone, hydroregion in INPUTS.items()\n",
    "    #     )\n",
    "# with parallel_config(backend=\"dask\", verbose=100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in ctl.query(\"run == 1\").iterrows():\n",
    "    \n",
    "    apm = \"\" if row.AppendMetric == \"none\" else row.AppendMetric\n",
    "    if row.use_mask == 1:\n",
    "        mask_dir = MASK_DIR_RP100\n",
    "    elif row.use_mask == 2:\n",
    "        mask_dir = MASK_DIR_SLP10\n",
    "    elif row.use_mask == 3:\n",
    "        mask_dir = MASK_DIR_SLP20\n",
    "    else:\n",
    "        mask_dir = \"\"\n",
    "    landscape_layer_year = re.findall(r'\\d+', row.LandscapeLayer)\n",
    "    actual_layer_name = f\"Annual_NLCD_LndCov_{landscape_layer_year[0]}_CU_C1V0.tif\"\n",
    "    layer = (\n",
    "        actual_layer_name\n",
    "        if \"/\" in row.LandscapeLayer or \"\\\\\" in row.LandscapeLayer\n",
    "        else (f\"{LYR_DIR}/{actual_layer_name}\")\n",
    "    )  # use abspath\n",
    "    # print(layer)\n",
    "    if isinstance(row.summaryfield, str):\n",
    "        summary = row.summaryfield.split(\";\")\n",
    "    else:\n",
    "        summary = None\n",
    "    if row.accum_type == \"Point\":\n",
    "        # Load in point geopandas table and Pct_Full table\n",
    "        # TODO: script to create this PCT_FULL_FILE\n",
    "        pct_full = pd.read_csv(\n",
    "            PCT_FULL_FILE if row.use_mask == 0 else PCT_FULL_FILE_RP100\n",
    "        )\n",
    "        points = gpd.read_file(layer)\n",
    "        if mask_dir:\n",
    "            points = mask_points(points, mask_dir, INPUTS)\n",
    "    # File string to store InterVPUs needed for adjustments\n",
    "    Connector = f\"{OUT_DIR}/{row.FullTableName}_connectors.csv\"\n",
    "    print(\n",
    "        f\"Acquiring `{row.FullTableName}` catchment statistics...\",\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )\n",
    "    #for zone, hydroregion in INPUTS.items():\n",
    "    def process_zone(zone, hydroregion, client):\n",
    "        zone_start_time = time.time()\n",
    "        if not os.path.exists(f\"{OUT_DIR}/{row.FullTableName}_{zone}.csv\"):\n",
    "            print(zone, end=\", \")\n",
    "            pre = f\"{NHD_DIR}/NHDPlus{hydroregion}/NHDPlus{zone}\"\n",
    "            if not row.accum_type == \"Point\":\n",
    "                izd = (\n",
    "                    f\"{mask_dir}/{zone}.tif\"\n",
    "                    if mask_dir\n",
    "                    else f\"{pre}/NHDPlusCatchment/cat\"\n",
    "                )\n",
    "                cat = createCatStats(\n",
    "                    row.accum_type,\n",
    "                    layer,\n",
    "                    izd,\n",
    "                    OUT_DIR,\n",
    "                    zone,\n",
    "                    row.by_RPU,\n",
    "                    mask_dir,\n",
    "                    NHD_DIR,\n",
    "                    hydroregion,\n",
    "                    apm,\n",
    "                    use_dask=True,\n",
    "                    dask_client=client\n",
    "                )\n",
    "            if row.accum_type == \"Point\":\n",
    "                izd = f\"{pre}/NHDPlusCatchment/Catchment.shp\"\n",
    "                cat = PointInPoly(\n",
    "                    points, zone, izd, pct_full, mask_dir, apm, summary, use_dask=True\n",
    "                )\n",
    "            cat.to_csv(f\"{OUT_DIR}/{row.FullTableName}_{zone}.csv\", index=False)\n",
    "            zone_end_time = time.time()\n",
    "            print(f\"Time to finish processing stats for zone {zone} / region {hydroregion} {(zone_end_time - zone_start_time) / 60} minutes\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with parallel_backend('dask'):\n",
    "        zone_results = Parallel(n_jobs=8, verbose=100)(\n",
    "            delayed(process_zone) (zone, hydroregion, client) for zone, hydroregion in INPUTS.items()\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    print(f\"Processed {len(INPUTS)} in {end_time - start_time} seconds with {os.cpu_count()} parallel processes\")\n",
    "    print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zone(zone, hydroregion, client):\n",
    "    if not os.path.exists(f\"{OUT_DIR}/{row.FullTableName}_{zone}.csv\"):\n",
    "        print(zone, end=\", \")\n",
    "        pre = f\"{NHD_DIR}/NHDPlus{hydroregion}/NHDPlus{zone}\"\n",
    "        if not row.accum_type == \"Point\":\n",
    "            izd = (\n",
    "                f\"{mask_dir}/{zone}.tif\"\n",
    "                if mask_dir\n",
    "                else f\"{pre}/NHDPlusCatchment/cat\"\n",
    "            )\n",
    "            cat = createCatStats(\n",
    "                row.accum_type,\n",
    "                layer,\n",
    "                izd,\n",
    "                OUT_DIR,\n",
    "                zone,\n",
    "                row.by_RPU,\n",
    "                mask_dir,\n",
    "                NHD_DIR,\n",
    "                hydroregion,\n",
    "                apm,\n",
    "                use_dask=True,\n",
    "                dask_client=client,\n",
    "            )\n",
    "        if row.accum_type == \"Point\":\n",
    "            izd = f\"{pre}/NHDPlusCatchment/Catchment.shp\"\n",
    "            cat = PointInPoly(\n",
    "                points, zone, izd, pct_full, mask_dir, apm, summary, use_dask=True, dask_client=client\n",
    "            )\n",
    "        cat.to_csv(f\"{OUT_DIR}/{row.FullTableName}_{zone}.csv\", index=False)\n",
    "\n",
    "# Step 3: Combine Joblib and Dask\n",
    "for _, row in ctl.query(\"run == 1\").iterrows():\n",
    "    apm = \"\" if row.AppendMetric == \"none\" else row.AppendMetric\n",
    "    if row.use_mask == 1:\n",
    "        mask_dir = MASK_DIR_RP100\n",
    "    elif row.use_mask == 2:\n",
    "        mask_dir = MASK_DIR_SLP10\n",
    "    elif row.use_mask == 3:\n",
    "        mask_dir = MASK_DIR_SLP20\n",
    "    else:\n",
    "        mask_dir = \"\"\n",
    "    landscape_layer_year = re.findall(r'\\d+', row.LandscapeLayer)\n",
    "    actual_layer_name = f\"Annual_NLCD_LndCov_{landscape_layer_year[0]}_CU_C1V0.tif\"\n",
    "    layer = (\n",
    "        actual_layer_name\n",
    "        if \"/\" in row.LandscapeLayer or \"\\\\\" in row.LandscapeLayer\n",
    "        else (f\"{LYR_DIR}/{actual_layer_name}\")\n",
    "    )  # use abspath\n",
    "    # print(layer)\n",
    "    if isinstance(row.summaryfield, str):\n",
    "        summary = row.summaryfield.split(\";\")\n",
    "    else:\n",
    "        summary = None\n",
    "    if row.accum_type == \"Point\":\n",
    "        pct_full = pd.read_csv(\n",
    "            PCT_FULL_FILE if row.use_mask == 0 else PCT_FULL_FILE_RP100\n",
    "        )\n",
    "        points = gpd.read_file(layer)\n",
    "        if mask_dir:\n",
    "            points = mask_points(points, mask_dir, INPUTS)\n",
    "    Connector = f\"{OUT_DIR}/{row.FullTableName}_connectors.csv\"\n",
    "    print(f\"Acquiring `{row.FullTableName}` catchment statistics...\", end=\"\", flush=True)\n",
    "    \n",
    "    with parallel_backend('dask'):\n",
    "        start_time = time.time()\n",
    "        zone_results = Parallel(n_jobs=8, verbose=100)(\n",
    "            delayed(process_zone)(zone, hydroregion, client) for zone, hydroregion in INPUTS.items()\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        print(f\"Processed {len(INPUTS)} zones in {end_time - start_time} seconds using Joblib + Dask\")\n",
    "        print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
