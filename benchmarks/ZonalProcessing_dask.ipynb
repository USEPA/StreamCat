{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://127.0.0.1:8787/status'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 13:38:42,518 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:62663 (pid=25960) exceeded 95% memory budget. Restarting...\n",
      "2025-03-11 13:38:43,934 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:62663' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('getitem-1861ac82fee65c3e8f8becb5ab641923', 3, 0), 'finalize-1e4898cd-39fe-40ca-86ce-23a042adf3ed', ('getitem-1861ac82fee65c3e8f8becb5ab641923', 16, 0), 'finalize-5f883999-61bc-4ecf-99ef-df2818888341', 'finalize-7b78f086-3d5f-4877-8ce9-89dbfff47b47', ('getitem-1861ac82fee65c3e8f8becb5ab641923', 40, 0), 'finalize-a4dcd0f2-a1e9-4148-8651-08addc47c975', 'finalize-c52d9e3b-73e7-4a1b-a5d5-27cca28df16d', 'finalize-6241984a-ae01-4b25-ba1e-e920802302b0', 'finalize-182c278a-8c54-4f08-8605-bd282d9649fa', 'finalize-16a4039c-ced0-4c63-ab65-222fa4ae05f1', 'finalize-35afcd37-5df5-4a7e-bfe2-c9966da57709', 'finalize-9af8a516-8b36-41c1-8d00-84d710a2e59c', 'finalize-96fc9f19-0deb-4123-aaa3-e79fc2d35388', 'finalize-97585e82-57b7-4cd5-88e1-bd9194ece052', 'finalize-8cfc2c3d-c0f9-47c1-84ff-8be05c604cfb', 'finalize-08eb1254-380a-4a4a-b5d9-0d0d753f7ef8', 'finalize-91b700ae-a302-411b-8012-460dbfa17bda', 'finalize-9b0733a0-4b75-44d0-8b37-8cffebe27ede', 'finalize-bea33045-8cc1-4a7e-8e0e-8a19788ed1d7', 'finalize-72d9cfba-3a31-483f-946f-7a01d4de9f7c', ('getitem-1861ac82fee65c3e8f8becb5ab641923', 37, 0), 'finalize-f9fbc2d1-caa1-4ffb-93fa-9de3a53e2c36', 'finalize-569011c8-71f7-43ad-a4a7-4c3780219e50', ('getitem-1861ac82fee65c3e8f8becb5ab641923', 30, 0), 'finalize-67096845-7373-4c18-b999-4c7e5951ef9c', 'finalize-e8b92f34-9af1-4ea1-8260-c6acafe0f7b4', ('getitem-1861ac82fee65c3e8f8becb5ab641923', 34, 0), 'finalize-d52286f8-db69-4c90-a924-aeb18ab17237', 'finalize-a45e4cd9-9d69-45ac-9d2a-0c159d9914ce', 'finalize-f12044ab-e0de-4712-8915-dddf761d6e96', 'finalize-4238e214-17b7-4b3d-8408-adeb41b42900', 'finalize-1dc31456-b51e-4ba5-877c-4722ca93a608'} (stimulus_id='handle-worker-cleanup-1741725523.933872')\n",
      "2025-03-11 13:38:45,056 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-03-11 13:39:15,992 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-03-11 14:13:14,346 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='127.0.0.1:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\websocket.py\", line 938, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\web.py\", line 3301, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\bokeh\\server\\views\\ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired.\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired.\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-18050851' coro=<Client._gather.<locals>.wait() done, defined at C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:2396> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py\", line 2405, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=16)\n",
    "client = Client(cluster)\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from stream_cat_config import (\n",
    "    LYR_DIR,\n",
    "    MASK_DIR_RP100,\n",
    "    MASK_DIR_SLP10,\n",
    "    MASK_DIR_SLP20,\n",
    "    ACCUM_DIR,\n",
    "    NHD_DIR,\n",
    "    OUT_DIR,\n",
    "    PCT_FULL_FILE,\n",
    "    PCT_FULL_FILE_RP100\n",
    ")\n",
    "\n",
    "from StreamCat_functions import (\n",
    "    PointInPoly,\n",
    "    createCatStats,\n",
    "    mask_points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl = pd.read_csv('../ControlTable_StreamCat.csv')\n",
    "inter_vpu = pd.read_csv(\"../InterVPU.csv\")\n",
    "INPUTS = np.load(ACCUM_DIR +\"/vpu_inputs.npy\", allow_pickle=True).item()\n",
    "\n",
    "already_processed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'06': 'MS', '05': 'MS', '10U': 'MS', '10L': 'MS', '07': 'MS', '11': 'MS', '14': 'CO', '01': 'NE', '17': 'PN', '16': 'GB', '15': 'CO', '13': 'RG', '12': 'TX', '09': 'SR', '02': 'MA', '08': 'MS', '04': 'GL', '03W': 'SA', '03S': 'SA', '03N': 'SA', '18': 'CA'})\n",
      "21\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "print(INPUTS)\n",
    "print(len(INPUTS))\n",
    "print(len(ctl.loc[ctl['run'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:/PRIV/CPHEA/PESD/COR/CORFiles/Geospatial_Library_Projects/StreamCat/LandscapeRasters/QAComplete/Annual_NLCD_LndCov_1987_CU_C1V0.tif\n",
      "Acquiring `N_Human_Waste_1987` catchment statistics...09, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 1.38 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 09 / region SR 9.661166687806448 minutes\n",
      "02, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 1.02 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 02 / region MA 14.2870729525884 minutes\n",
      "08, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 681.67 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 08 / region MS 22.746868260701497 minutes\n",
      "04, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 1.47 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 04 / region GL 11.195889695485432 minutes\n",
      "03W, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 494.00 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 03W / region SA 6.366673819224039 minutes\n",
      "03S, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 713.90 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 03S / region SA 8.55884046157201 minutes\n",
      "03N, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 716.39 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 03N / region SA 4.151989034811655 minutes\n",
      "18, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 1.37 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 18 / region CA 10.597883538405101 minutes\n",
      "Processed 21 regions for one metric N_Human_Waste_1987 in 5253.993076324463 seconds using a dask and 8 workers\n",
      "done!\n",
      "O:/PRIV/CPHEA/PESD/COR/CORFiles/Geospatial_Library_Projects/StreamCat/LandscapeRasters/QAComplete/Annual_NLCD_LndCov_1988_CU_C1V0.tif\n",
      "Acquiring `N_Human_Waste_1988` catchment statistics...06, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 328.06 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 06 / region MS 2.3606559594472247 minutes\n",
      "05, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 1.00 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 05 / region MS 11.17708779970805 minutes\n",
      "10U, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 1.69 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to finish processing stats for zone 10U / region MS 36.679597775141396 minutes\n",
      "10L, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thudso02\\AppData\\Roaming\\Python\\Python312\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 1.46 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m row\u001b[38;5;241m.\u001b[39maccum_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPoint\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     47\u001b[0m     izd \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzone\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask_dir\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/NHDPlusCatchment/cat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m     )\n\u001b[1;32m---> 52\u001b[0m     cat \u001b[38;5;241m=\u001b[39m \u001b[43mcreateCatStats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccum_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mizd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mby_RPU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mNHD_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhydroregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_dask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row\u001b[38;5;241m.\u001b[39maccum_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPoint\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     66\u001b[0m     izd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/NHDPlusCatchment/Catchment.shp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\thudso02\\repositories\\parallel_streamcat\\StreamCat\\StreamCat_functions.py:1147\u001b[0m, in \u001b[0;36mcreateCatStats\u001b[1;34m(accum_type, LandscapeLayer, inZoneData, out_dir, zone, by_RPU, mask_dir, NHD_dir, hydroregion, appendMetric, use_dask)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m#print(outTable)\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_dask:\n\u001b[1;32m-> 1147\u001b[0m     outTable \u001b[38;5;241m=\u001b[39m \u001b[43moutTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;66;03m# if use_dask and dask_client is not None:\u001b[39;00m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;66;03m#     outTable = dask_client.compute(outTable) # [0] # outTable.compute()\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;66;03m#print(outTable.head())\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m \n\u001b[0;32m   1152\u001b[0m \u001b[38;5;66;03m# Post process dataframe\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outTable\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask_expr\\_collection.py:477\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    476\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\dask\\base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\threading.py:634\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    632\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 634\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\threading.py:338\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 338\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    340\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _, row in ctl.query(\"run == 1\").iterrows():\n",
    "\n",
    "    apm = \"\" if row.AppendMetric == \"none\" else row.AppendMetric\n",
    "    if row.use_mask == 1:\n",
    "        mask_dir = MASK_DIR_RP100\n",
    "    elif row.use_mask == 2:\n",
    "        mask_dir = MASK_DIR_SLP10\n",
    "    elif row.use_mask == 3:\n",
    "        mask_dir = MASK_DIR_SLP20\n",
    "    else:\n",
    "        mask_dir = \"\"\n",
    "    landscape_layer_year = re.findall(r'\\d+', row.LandscapeLayer)\n",
    "    actual_layer_name = f\"Annual_NLCD_LndCov_{landscape_layer_year[0]}_CU_C1V0.tif\"\n",
    "    layer = (\n",
    "        actual_layer_name\n",
    "        if \"/\" in row.LandscapeLayer or \"\\\\\" in row.LandscapeLayer\n",
    "        else (f\"{LYR_DIR}/{actual_layer_name}\")\n",
    "    )  # use abspath\n",
    "    print(layer)\n",
    "    if isinstance(row.summaryfield, str):\n",
    "        summary = row.summaryfield.split(\";\")\n",
    "    else:\n",
    "        summary = None\n",
    "    if row.accum_type == \"Point\":\n",
    "        # Load in point geopandas table and Pct_Full table\n",
    "        # TODO: script to create this PCT_FULL_FILE\n",
    "        pct_full = pd.read_csv(\n",
    "            PCT_FULL_FILE if row.use_mask == 0 else PCT_FULL_FILE_RP100\n",
    "        )\n",
    "        points = gpd.read_file(layer)\n",
    "        if mask_dir:\n",
    "            points = mask_points(points, mask_dir, INPUTS)\n",
    "    # File string to store InterVPUs needed for adjustments\n",
    "    Connector = f\"{OUT_DIR}/{row.FullTableName}_connectors.csv\"\n",
    "    print(\n",
    "        f\"Acquiring `{row.FullTableName}` catchment statistics...\",\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    for zone, hydroregion in INPUTS.items():\n",
    "        zone_start_time = time.time()\n",
    "        if not os.path.exists(f\"{OUT_DIR}/{row.FullTableName}_{zone}.csv\"):\n",
    "            print(zone, end=\", \", flush=True)\n",
    "            pre = f\"{NHD_DIR}/NHDPlus{hydroregion}/NHDPlus{zone}\"\n",
    "            if not row.accum_type == \"Point\":\n",
    "                izd = (\n",
    "                    f\"{mask_dir}/{zone}.tif\"\n",
    "                    if mask_dir\n",
    "                    else f\"{pre}/NHDPlusCatchment/cat\"\n",
    "                )\n",
    "                cat = createCatStats(\n",
    "                    row.accum_type,\n",
    "                    layer,\n",
    "                    izd,\n",
    "                    OUT_DIR,\n",
    "                    zone,\n",
    "                    row.by_RPU,\n",
    "                    mask_dir,\n",
    "                    NHD_DIR,\n",
    "                    hydroregion,\n",
    "                    apm,\n",
    "                    use_dask=True\n",
    "                )\n",
    "            if row.accum_type == \"Point\":\n",
    "                izd = f\"{pre}/NHDPlusCatchment/Catchment.shp\"\n",
    "                cat = PointInPoly(\n",
    "                    points, zone, izd, pct_full, mask_dir, apm, summary, use_dask=True\n",
    "                )\n",
    "            cat.to_csv(f\"{OUT_DIR}/{row.FullTableName}_{zone}.csv\", index=False)\n",
    "            zone_end_time = time.time()\n",
    "            print(f\"Time to finish processing stats for zone {zone} / region {hydroregion} {(zone_end_time - zone_start_time) / 60} minutes\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Processed {len(INPUTS)} regions for one metric {row.FullTableName} in {end_time-start_time} seconds using a dask and 8 workers\")\n",
    "    print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
